---
title: "AML Assignment 2"
subtitle: "Yiting Wang, Tyler Deroin, Sarah Laouiti"
author: "CID: 01423116, 01404042, 01429506"
date: "2018-03-06"
output: html_document
---

## Question 1

### (i)

$X^TX$ is a $(d+1) * (d+1)$ matrix and can be written as a sum of the form $\sum_{i=1}^N z_i^Tz_i$ where $z_i$ is a 1 * (d + 1) vector:
$$z_i = \left[
 \begin{matrix}
   1 & x_{i1} & x_{i2} & ... & x_{id}
  \end{matrix}
  \right]$$
$z_i$ represents one data point. We can query every data point from 1 to N from the database, calculate $z_i^Tz_i$ and then sum all of them to get $X^TX$.


### (ii)

First, for $X^TX$, we need to keep only $O(d^2)$ numbers in the database. According to (i), when a new datapoint $x_k$ arrives at time $t_k$, to get an updated $X^TX$, we only need to add $z_k^Tz_k$ on the previous $X^TX$. $X^TX$ is still a $(d+1) * (d+1)$ matrix, and the memory for saving $X^TX$ is always $O(d^2)$.

Second, for $X^Ty$,we also need to keep only $O(d^2)$ numbers in the database. $X^Ty$ is a $(d+1)*1$ matrix and can be written as a sum of the form $\sum_{i=1}^N z_i^Ty_i$. Similarly, when a new datapoint $x_k$ arrives at time $t_k$, to get an updated $X^Ty$, we only need to add $z_k^Ty_k$ on the previous $X^Ty$. $X^Ty$ is still a $(d+1) * 1$ matrix, and the memory for saving $X^Ty$ is always $O(d)$.

Since $\beta^* = (X^TX+ \lambda I)^{-1}X^Ty$, and $X^TX$ and $X^Ty$ can be save in the database with only $O(d^2)$ numbers, even the data points grow sequentially, we find a way to compute $\beta^*$ while keeping only $O(d^2)$ numbers in the database.

### (iii)

Let $M_N = (X^TX + \lambda I)$, $M_N$ is a $(d+1) * (d+1)$ matrix. $M_N$ can be written as:
$$M_N = \sum_{i=1}^N z_i^Tz_i + \lambda I$$

$M_{N+1}$ can be written as:
$$M_{N+1} = \sum_{i=1}^N z_i^Tz_i + z_{N+1}^T\space z_{N+1} + \lambda I = M_N + z_{N+1}^T\space z_{N+1}$$
According to Sherman-Morrison-Woodbury identity, $(M_{N+1})^{-1}$ can be written as:

$$(M_{N+1})^{-1} = (M_N)^{-1} - \frac{(M_N)^{-1}\space z_{N+1}^T\space z_{N+1}\space (M_N)^{-1}}{1 + z_{N+1}\space (M_N)^{-1}\space z_{N+1}^T}$$  
When a new observation $X_{N+1}$ is given, we can use the above formula to calcuate $(M_{N+1})^{-1}$. $(M_N)^{-1}$ is already computed in previous calculation. So$(M_{N+1})^{-1}$ can be computed within $O(d^2)$ time. (***not sure how to prove!!!)


## Question 2

### (i)

The city want to estimate $p$, the proportion of parked cars in violation of the meter, by $\hat p$, the fraction of such cars within the random sample. But this sampling scheme was not practical so the city decided to do stratified ramdom sampling. $\widetilde p$ is another approximation of $p$, representing the fraction of cars parking without paying the meters within the stratified random sample.

###(ii)

Stratified random sampling ensures each subgroup (in this case, block) within the population receives proper representation within the sample.

### (iii)
not reasonable 
www.promesa.co.nz/Help/EP_est_stratified_random_sample.htm

### (iv)
biased
need an example...

## Question 3

### (i)
```{r}
data = data.frame(
  x = c(4,3,3,3,2,4,4,3,2,1,3,2), 
  z = c(12,8,9,16,7,15,10,15,6,12,8,14))

estimate_p <- sum(data$x)/sum(data$z)
```

The city??s $\widetilde p$ of the citywide ??scofflaw?? rate $p$ is about $25.8%$.

### (ii)

### (iii)

### (iv)

```{r}

boot(data = data, statistic = , R = 1000)
```


### (v)

### (vi)



## Question 4
```{r, message=TRUE, include=FALSE}
library(readr)
library(ISLR)
library(boot)
library(tree)
library(glmnet)
library(plyr)
library(randomForest)
library(dplyr)
library(caret)
tahoe <- read.csv("Tahoe_Healthcare_Data.csv")
tahoe$readmit30 <- factor(tahoe$readmit30)

#Split 80% training data and 20% test data
set.seed(123)
sample <- sample.int(n = nrow(tahoe), size = floor(.8*nrow(tahoe)), replace = F)
train <- tahoe[sample,]
test <- tahoe[-sample,]
train_x <- as.matrix(train[1:6])
train_y <- as.matrix(train$readmit30)
test_x <- as.matrix(test[1:6])
test_y <- as.matrix(test$readmit30)
#5 fold cross validation

cv_splits <- createFolds(y = train_y, k = 5)
str(cv_splits)
control <- trainControl(method = "cv", number = 5)
#Report results on test data
```



### Logistic Regression
```{r}
set.seed(123)
cv.glmmod <- cv.glmnet(x = train_x, y=train_y, alpha=1, family = "binomial", nfolds = 5)

log.predict <- as.numeric(predict(cv.glmmod, newx = as.matrix(test_x), s = "lambda.min", type = "class"))
#compare predictions to test data
sum(log.predict == test_y) / length(test_y)
#confusion matrix 
table(log.predict, test_y)
```



### Classification Trees

```{r}
#Build base model
tree.model <- tree(readmit30~., data = train)
#tune under cross validation for number of nodes
cv.model <- cv.tree(object = tree.model, K = 5)
#Decide best size (# of nodes)
best.size <- cv.model$size[which(cv.model$dev==min(cv.model$dev))]
#prune based on best.size
cv.model.pruned <- prune.misclass(tree.model, best=best.size)
#predict with prune model
tree.predict <- predict(cv.model.pruned, as.data.frame(test_x), type="class")

plot(cv.model.pruned)
text(cv.model.pruned)
#Results
sum(tree.predict == test_y)/ length(test_y)
table(tree.predict, test_y)
```


### Bagging
```{r}
set.seed(123)
#Random forest function but mtry = number of explanatory variables
bag_grid <- expand.grid(vars = c(1,2,3,4,5,6))
caret::modelLookup(model = "bag")
caret::modelLookup(model = "gbm")

bag.cv <- train(readmit30~., data = train, method = "bag", trControl = control, tuneGrid = bag_grid)
bag.cv
bag(test_x, bagControl = ())




bag1 <- bagging(readmit30~., train, nbagg = 25)

```



### Random forests
```{r}
set.seed(123)
#possible values of mtry
rf_grid <- expand.grid(mtry= c(1,2,3,4,5,6))
#train under cv and possible mtry
rf <- train(readmit30~., data = train, method = "rf", tuneGrid = rf_grid, trControl = control)
#besttune for mtry value
final <- randomForest(readmit30~., data = train, mtry = as.numeric(rf$bestTune[1]))

#Predict
rf.predict <- predict(final, newdata = as.data.frame(test_x), type = "class")

#Results
sum(rf.predict == test_y)/length(test_y)
table(tree.predict, test_y)
```




