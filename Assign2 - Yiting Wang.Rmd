---
title: "Assign2"
author: "Yiting Wang"
date: "2018-Feb-28"
output: html_document
---

## Question 1

### (i)

$X^TX$ is a $(d+1) * (d+1)$ matrix and can be written as a sum of the form $\sum_{i=1}^N z_i^Tz_i$ where $z_i$ is a 1 * (d + 1) vector:
$$z_i = \left[
 \begin{matrix}
   1 & x_{i1} & x_{i2} & ... & x_{id}
  \end{matrix}
  \right]$$
$z_i$ represents one data point. We can query every data point from 1 to N from the database, calculate $z_i^Tz_i$ and then sum all of them to get $X^TX$.


### (ii)

First, for $X^TX$, we need to keep only $O(d^2)$ numbers in the database. According to (i), when a new datapoint $x_k$ arrives at time $t_k$, to get an updated $X^TX$, we only need to add $z_k^Tz_k$ on the previous $X^TX$. $X^TX$ is still a $(d+1) * (d+1)$ matrix, and the memory for saving $X^TX$ is always $O(d^2)$.

Second, for $X^Ty$,we also need to keep only $O(d^2)$ numbers in the database. $X^Ty$ is a $(d+1)*1$ matrix and can be written as a sum of the form $\sum_{i=1}^N z_i^Ty_i$. Similarly, when a new datapoint $x_k$ arrives at time $t_k$, to get an updated $X^Ty$, we only need to add $z_k^Ty_k$ on the previous $X^Ty$. $X^Ty$ is still a $(d+1) * 1$ matrix, and the memory for saving $X^Ty$ is always $O(d)$.

Since $\beta^* = (X^TX+ \lambda I)^{-1}X^Ty$, and $X^TX$ and $X^Ty$ can be save in the database with only $O(d^2)$ numbers, even the data points grow sequentially, we find a way to compute $\beta^*$ while keeping only $O(d^2)$ numbers in the database.

### (iii)

Let $M_N = (X^TX + \lambda I)$, $M_N$ is a $(d+1) * (d+1)$ matrix. $M_N$ can be written as:
$$M_N = \sum_{i=1}^N z_i^Tz_i + \lambda I$$

$M_{N+1}$ can be written as:
$$M_{N+1} = \sum_{i=1}^N z_i^Tz_i + z_{N+1}^T\space z_{N+1} + \lambda I = M_N + z_{N+1}^T\space z_{N+1}$$
According to Sherman-Morrison-Woodbury identity, $(M_{N+1})^{-1}$ can be written as:

$$(M_{N+1})^{-1} = (M_N)^{-1} - \frac{(M_N)^{-1}\space z_{N+1}^T\space z_{N+1}\space (M_N)^{-1}}{1 + z_{N+1}\space (M_N)^{-1}\space z_{N+1}^T}$$  
When a new observation $X_{N+1}$ is given, we can use the above formula to calcuate $(M_{N+1})^{-1}$. $(M_N)^{-1}$ is already computed in previous calculation. So$(M_{N+1})^{-1}$ can be computed within $O(d^2)$ time. 
(***actually... I am not sure how to prove it...)


## Question 2

### (i)

The city want to estimate $p$, the proportion of parked cars in violation of the meter, by $\hat p$, the fraction of such cars within the random sample. But this sampling scheme was not practical so the city decided to do stratified ramdom sampling. $\widetilde p$ is another approximation of $p$, representing the fraction of cars parking without paying the meters within the stratified random sample.

###(ii)

Stratified random sampling ensures each subgroup (in this case, block) within the population receives proper representation within the sample.

### (iii)
We believe it is not reasonable to apply the usual formulas suitable for purely random sample.
www.promesa.co.nz/Help/EP_est_stratified_random_sample.htm

### (iv)

```{r}
sample <- data.frame(x = c(3,4,6), z = c(12,14,16))
sample1 <- data.frame(x = c(3,4), z = c(12,14))
sample2 <- data.frame(x = c(3,6), z = c(12,16))
sample3 <- data.frame(x = c(4,6), z = c(14,16))

p <- sum(sample$x)/sum(sample$z)
p

p1 <- sum(sample1$x)/sum(sample1$z)
p1

p2 <- sum(sample2$x)/sum(sample2$z)
p2

p3 <- sum(sample3$x)/sum(sample3$z)
p3

expected_p <- (p1 + p2 + p3)/3
expected_p

expected_p == p
```
Imagine a city that has three blocks with parking meters, two of which will be sampled at random. The true $p$ can be computed using all three blocks' data. The $E[\widetilde p]$ can be computed using the average of three $\widetilde p$ from three different samples(block 1&2, block 1&3, block2&3). Although the two numbers are very close, they are not the same. So $\widetilde p$ is not an unbiased estimator for p.

## Question 3

### (i)
```{r}
data = data.frame(
  x = c(4,3,3,3,2,4,4,3,2,1,3,2), 
  z = c(12,8,9,16,7,15,10,15,6,12,8,14))

estimate_p <- sum(data$x)/sum(data$z)
estimate_p 
```

The city¡¯s $\widetilde p$ of the citywide ¡°scofflaw¡± rate $p$ is about $25.8%$.

### (ii)

Since we do not know the true data for calculating $p$, we use bootstrapping to estimate the variability of a parameter estimate, in this case, calculating the level of sampling error (mean, std deviation...).

"If one estimated the citywide ¡°scofflaw¡± rate by the ratio ¡°cars in violation/cars at meters¡± for all the blocks combined, what level of sampling error attends the result? That is a fairly straightforward problem with bootstrapping, but quite an intractable one without it."

link: https://www.informs.org/ORMS-Today/Public-Articles/June-Volume-42-Number-3/Innovative-Education-Probability-as-a-reality-show


### (iii)
???

### (iv)

```{r}
library(boot)

boot.fn = function(data,index){
  x = data$x[index]
  z = data$z[index]
  return(sum(x)/sum(z))
}

set.seed(1)
boot_result <- boot(data, boot.fn, 1000)
boot_result
```
whether the estimation procefure suffers an appreciable bias???

###(v)

```{r}
SE <- 0.02864137
lower <- estimate_p - 1.96*SE
upper <- estimate_p + 1.96*SE
cat("95% confidence interval for p is [", lower, ",", upper , "]" )
```

### (vi)

```{r}

```
cannot understand???


## Question 4

```{r}
# load data
hospital <- read.csv("C:/Tahoe_Healthcare_Data.csv")
summary(hospital)
```

```{r}
# split data
test_n <- round(0.8*nrow(hospital))

train_x <- as.matrix(hospital[1:test_n, 1:6])
train_y <- as.matrix(hospital[1:test_n, 7])
test_x <- as.matrix(hospital[(test_n + 1):nrow(hospital), 1:6])
test_y <- as.matrix(hospital[(test_n + 1):nrow(hospital), 7])
```

```{r}
# 1. lasso + logistic regression
library(glmnet)

set.seed(1)
cvfit1 <- cv.glmnet(train_x, train_y, nfolds = 5, alpha = 1, family = "binomial")

p1 <- as.numeric(predict(cvfit1, newx = test_x, s = "lambda.min", type = "class", family = "binomial"))

# real readmission rate for test data
sum(test_y)/nrow(test_y)

# estimated readmission rate for test data
sum(p1)/nrow(test_y)

# performance on test set
sum(p1 == test_y) / nrow(test_y)

# confusion matrix
table(p1, test_y)
```


```{r}
# 2. classification trees

```

```{r}
# 3. bagging

```

```{r}
# 4. random forest

```


