---
title: "AML Assignment 2"
subtitle: "Yiting Wang, Tyler Deroin, Sarah Laouiti"
author: "CID: 01423116, 01404042, 01429506"
date: "2018-03-06"
output: html_document
---

## Question 1

### (i)

$X^TX$ is a $(d+1) * (d+1)$ matrix and can be written as a sum of the form $\sum_{i=1}^N z_i^Tz_i$ where $z_i$ is a 1 * (d + 1) vector:
$$z_i = \left[
 \begin{matrix}
   1 & x_{i1} & x_{i2} & ... & x_{id}
  \end{matrix}
  \right]$$
$z_i$ represents one data point. We can query every data point from 1 to N from the database, calculate $z_i^Tz_i$ and then sum all of them to get $X^TX$.


### (ii)

First, for $X^TX$, we need to keep only $O(d^2)$ numbers in the database. According to (i), when a new datapoint $x_k$ arrives at time $t_k$, to get an updated $X^TX$, we only need to add $z_k^Tz_k$ on the previous $X^TX$. $X^TX$ is still a $(d+1) * (d+1)$ matrix, and the memory for saving $X^TX$ is always $O(d^2)$.

Second, for $X^Ty$,we also need to keep only $O(d^2)$ numbers in the database. $X^Ty$ is a $(d+1)*1$ matrix and can be written as a sum of the form $\sum_{i=1}^N z_i^Ty_i$. Similarly, when a new datapoint $x_k$ arrives at time $t_k$, to get an updated $X^Ty$, we only need to add $z_k^Ty_k$ on the previous $X^Ty$. $X^Ty$ is still a $(d+1) * 1$ matrix, and the memory for saving $X^Ty$ is always $O(d)$.

Since $\beta^* = (X^TX+ \lambda I)^{-1}X^Ty$, and $X^TX$ and $X^Ty$ can be save in the database with only $O(d^2)$ numbers, even the data points grow sequentially, we find a way to compute $\beta^*$ while keeping only $O(d^2)$ numbers in the database.

### (iii)

Let $M_N = (X^TX + \lambda I)$, $M_N$ is a $(d+1) * (d+1)$ matrix. $M_N$ can be written as:
$$M_N = \sum_{i=1}^N z_i^Tz_i + \lambda I$$

$M_{N+1}$ can be written as:
$$M_{N+1} = \sum_{i=1}^N z_i^Tz_i + z_{N+1}^T\space z_{N+1} + \lambda I = M_N + z_{N+1}^T\space z_{N+1}$$
According to Sherman-Morrison-Woodbury identity, $(M_{N+1})^{-1}$ can be written as:

$$(M_{N+1})^{-1} = (M_N)^{-1} - \frac{(M_N)^{-1}\space z_{N+1}^T\space z_{N+1}\space (M_N)^{-1}}{1 + z_{N+1}\space (M_N)^{-1}\space z_{N+1}^T}$$  
When a new observation $X_{N+1}$ is given, we can use the above formula to calcuate $(M_{N+1})^{-1}$. $(M_N)^{-1}$ is already computed in previous calculation. So$(M_{N+1})^{-1}$ can be computed within $O(d^2)$ time. (***not sure how to prove!!!)


## Question 2

### (i)

The city want to estimate $p$, the proportion of parked cars in violation of the meter, by $\hat p$, the fraction of such cars within the random sample. But this sampling scheme was not practical so the city decided to do stratified ramdom sampling. $\widetilde p$ is another approximation of $p$, representing the fraction of cars parking without paying the meters within the stratified random sample.

###(ii)

Stratified random sampling ensures each subgroup (in this case, block) within the population receives proper representation within the sample.

### (iii)
not reasonable 
www.promesa.co.nz/Help/EP_est_stratified_random_sample.htm

### (iv)
biased
need an example...

## Question 3

### (i)
```{r}
data = data.frame(
  x = c(4,3,3,3,2,4,4,3,2,1,3,2), 
  z = c(12,8,9,16,7,15,10,15,6,12,8,14))

estimate_p <- sum(data$x)/sum(data$z)
```

The city??s $\widetilde p$ of the citywide ??scofflaw?? rate $p$ is about $25.8%$.

### (ii)

### (iii)

### (iv)

```{r}

boot(data = data, statistic = , R = 1000)
```


### (v)

### (vi)



## Question 4
```{r, message=TRUE, include=FALSE}
library(readr)
library(ISLR)
library(boot)
library(tree)
library(glmnet)
library(randomForest)
library(dplyr)
tahoe <- read_csv("Tahoe_Healthcare_Data.csv")

#Split 80% training data and 20% test data
set.seed(123)
sample <- sample.int(n = nrow(tahoe), size = floor(.8*nrow(tahoe)), replace = F)
train <- tahoe[sample,]
test <- tahoe[-sample,]
train_x <- train %>% select(-readmit30)
train_y <- train$readmit30
test_x <- test %>% select(-readmit30)
test_y <- train$readmit30
#5 fold cross validation
#Report results on test data
```



### Logistic Regression
```{r}
cv.glmmod <- cv.glmnet(as.matrix(train_x), y=train_y, alpha=1, nfolds = 5)
plot(cv.glmmod)
```



### Classification Trees

```{r}
#Example in chapter 8


cv.tree
```


### Bagging
```{r}
#Chapter 8
```



### Random forests
```{r}
#Chapter 8

```

