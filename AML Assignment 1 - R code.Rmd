---
title: "AML Assignment 1"
subtitle: "Yiting Wang, Tyler Deroin, Sarah Laouiti"
author: "CID: 01423116, 01404042, 01429506"
date: "2018-02-22"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(readr)
library(dplyr)


```

```{r data, warning=FALSE, message=FALSE, include=FALSE}
q1_a <- read_csv("q1_a.csv")
q1_b <- read_csv("q1_b.csv")
q1_c <- read_csv("q1_c.csv")
```

## Question 1
For (a), we define a function as $y_1 = \sqrt{x_1^2 + x_2^2}$ and then plot the new space as $x_1$ and $y_1$.

```{r part_a}
#Initial
ggplot(q1_a, aes(x = x1, y = x2, col = col, shape = col))+ geom_point() + ggtitle("Initial A") + scale_color_manual(values = c("black","blue")) + theme(legend.position = "none") + scale_shape_manual(values = c(18,15 ))

#Transform
a_new <- q1_a %>% mutate(y1 = sqrt((x1 * x1) + (x2 * x2)))

#New plot with line
ggplot(a_new, aes(x = x1, y = y1,  col = col, shape = col)) + geom_point() + ggtitle("New A") + scale_color_manual(values = c("black","blue")) + theme(legend.position = "none") + geom_abline(slope = 0, intercept = 2, color = "red") + scale_shape_manual(values = c(18,15 ))
```

For (b), we define a function for $y_1$ as $y_1 = x1 * x2$ giving us a new space as $x_1$ and $y_1$.

```{r part_b}
#Initial
ggplot(q1_b, aes(x = x1, y = x2, col = col, shape = col))+ geom_point() + ggtitle("Initial B") + scale_color_manual(values = c("black","blue")) + theme(legend.position = "none") + scale_shape_manual(values = c(18,15 ))

#Transform
b_new <- q1_b %>%  mutate(y1 = x1 * x2)

#New plot with line
ggplot(b_new, aes(x = x1, y = y1, col = col, shape = col)) + geom_point() + ggtitle("New B") + scale_color_manual(values = c("black","blue")) + theme(legend.position = "none") + geom_abline(slope = 0, intercept = 0, color = "red") + scale_shape_manual(values = c(18,15 ))
```

For (c), we follow a similar process with the function $y_1 = x_1 * x_1$.

```{r part_c}
#Initial
ggplot(q1_c, aes(x = x1, y = x2, col = col, shape = col))+ geom_point() + ggtitle("Initial C") + scale_color_manual(values = c("black","blue")) + theme(legend.position = "none") + scale_shape_manual(values = c(18,15 ))

#Transform
c_new <- q1_c %>% mutate(y1 = x1 * x1) 

#New plot with line
ggplot(c_new, aes(x = x1, y = y1, col = col, shape = col)) + geom_point() + ggtitle("New C") + scale_color_manual(values = c("black","blue")) + scale_shape_manual(values = c(18,15 )) + theme(legend.position = "none") + geom_abline(slope = 0, intercept = 2, color = "red")
```


## Question 2

```{r, include = FALSE}
# load data
data <- read.csv("Tahoe_Healthcare_Data.csv")
#View(data)
```

### (i)
```{r}
readmit <- sum(data$readmit30)
readmit
unit_loss <- 8000
total_loss <- readmit * unit_loss
total_loss
```

We take the dataset as representative of what will happen in a given year if nothing is done to reduce the readmissions rate. From the dataset, we can calculate the total number of re-admitted patients, which is 998. We mutiply this number by the loss per re-admitted patient \$8,000, and get the total lost of $7,984,000.

### (ii)
```{r}
program_unit_cost <- 1200
patients <- nrow(data)
patients 
program_total_cost <- patients * program_unit_cost
program_total_cost

reduce_rate <- 0.4
saving <- total_loss * reduce_rate
saving

net_change <- program_total_cost - saving
net_change
```

From the dataset, we know there are total 4,382 patients in a given year and the cost of CareTracker program is \$1,200 per patient. Therefore, the hospital implements CareTracker for all patients, the total cost will be $5,258,400, which is a quite high figure. 

And we know CareTracker reduced the incidence of readmissions by 40% in the pilot study, which means it can save 40% of the total loss we calculated in problem (i). So the hospital can save $3,193,600.

The net change in cost is \$2,064,800. If the hospital implements CareTracker for each patient, the cost compared to status quo, will increase $2,064,800, which means the money hospital spends on implementing CareTracker is higher than the money it can save from CareTracker's results. 

From an economic point of view, the hospital should not implement CareTracker for all AMI patients. Maybe it should choose some particular patients to implement CareTracker.

### (iii)
```{r}
best_saving <- reduce_rate * readmit * unit_loss - readmit * program_unit_cost 
best_saving
```

Suppose the hospital can find all the 998 re-admitted patients correctly, and implement CareTracker to them (cost is \$1,200 per patient), which will cost \$1,197,600. However, 40% of the 998 re-admitted patients will be free from readmission, and each will save \$8,000, so the total saving is \$3,193,600. So the upper bound on the possible savings should be $1,996,000. The hospital cannot expect to make more savings by implementing CareTracker.

### (iv)
```{r}
outcome = matrix(NA, nrow = 76, ncol = 2)

for (s in 25:100) {
  data_new <- data %>% mutate(estimate = ifelse(severity.score > s, 1, 0))
  cost <- 1200 * sum(data_new$estimate)
  save <- (sum(data_new$readmit30 + data_new$estimate == 2)) * 0.4 * 8000
  net <- save - cost
  outcome[s - 24, 1] = s
  outcome[s - 24, 2] = net
}

plot(outcome[,1], outcome[,2], type = "l", xlab = "S*", ylab = "savings")

max_saving <- max(outcome[,2])
max_saving

#Cost savings where S* = 50
outcome[26,][2]

best_s <- outcome[,1][which(outcome[,2] == max_saving)]
best_s
```

From the graph and outcome matrix, we can find out that the best value for the threshold S* is 41, under this circumstance, hospital can obtain a cost savings of $136,800 over the status quo.


### (v)
```{r}
glm.fit <- glm(readmit30 ~ age + female + flu_season + ed_admit + severity.score + comorbidity.score, data = data, family = binomial)

summary(glm.fit)

```


### (vi)
```{r}
data <- data %>% mutate(fit = predict(glm.fit, data, type = "response"))

outcome2 <- matrix(NA, nrow = 81, ncol = 2)

n = 0

for (p in seq(0.1, 0.9, by = 0.01)) {
  data_new <- data %>% mutate(estimate = ifelse(fit > p, 1, 0))
  cost <- 1200 * sum(data_new$estimate)
  save <- (sum(data_new$readmit30 + data_new$estimate == 2)) * 0.4 * 8000
  net <- save - cost
  n = n + 1
  outcome2[n, 1] = p
  outcome2[n, 2] = net
}

plot(outcome2[,1], outcome2[,2], type = "l", xlab = "p*", ylab = "savings")

max_saving2 <- max(outcome2[,2])
max_saving2

#outcome of p* = .6
outcome2[51,][2]

best_s <- outcome2[,1][which(outcome2[,2] == max_saving2)]
best_s
```

From the graph and outcome matrix, we can find out that the best value for the threshold p* is 0.4, under this circumstance, hospital can obtain a cost savings of $495,200 over the status quo.


## Question 3

Suppose we esimate the regression coefficients in a linear regression model by minimizing 

$$\sum_{i = 1}^n(y_i - \beta_0 - \sum_{j-1}^p \beta_j x_{ij})^2 + \lambda \sum_{j=1}^p \beta_j^2$$
for a particular $\lambda$.  For parts (a) through (e), indicate which of (i) through (V) is correct. Justify your answer



(a) As we increase $\lambda$ from 0, the training RSS will:

$Steadily \space increase.$ $\lambda$ is a tuning paramater which shrinks the $\beta$ coefficients thus placing strain on the model. As it increases, given the RSS equation, the smaller the $\beta$ coefficients will become and as a result the RSS will become larger.
    
(b) Repeat (a) for test RSS.

$Decrease \space initially, \space and \space then \space eventually \space start \space increasing \space in \space a \space U \space shape.$ At first, 'false' coefficients are forced to 0 and test RSS will decrease as the resulting model overfits less. However, there will reach a point where those irrelevant coefficients will have all been removed from the model, and, as per a), the RSS will increase.  Additionally, as the penalty term removes variance and introduces bias.  Initially, this is effective as more variance is removed than bias introduced. Eventually, bias introduced is greater than the variance removed and test RSS begins increasing.

(c) Repeat (a) for variance (of predicted values on test data where the x's are random).

$Steadily \space decrease.$ As $\lambda$ increases and $\beta$ coefficients decrease, the less complex the model. In other words, the flexibility of the model fit decreases and variance decreases.

(d) Repeat (a) for (squared) bias.

$Steadily \space increase$. As per c), variance would increase as $\lambda$ increases. Given the innate trade-off between variance and bias, it is expected that as variance decreases bias would increase.

(e) Repeat (a) for the irreducible error.

$Remain \space constant.$ As stated by the name, the irreducible error cannot be reduced by model selection or coefficients.
